@misc{.ShroomID.2023,
  title = {{{ShroomID}}},
  year = {2023},
  month = nov,
  urldate = {2024-06-05}
}

@article{BarreStoverEtAl.LeafNetComputerVision.2017,
  title = {{{LeafNet}}: {{A}} Computer Vision System for Automatic Plant Species Identification},
  author = {Barr{\'e}, Pierre and St{\"o}ver, Ben C. and M{\"u}ller, Kai F. and Steinhage, Volker},
  year = {2017},
  month = jul,
  journal = {Ecological Informatics},
  volume = {40},
  pages = {50--56},
  issn = {1574-9541},
  doi = {10.1016/j.ecoinf.2017.05.005},
  abstract = {Aims Taxon identification is an important step in many plant ecological studies. Its efficiency and reproducibility might greatly benefit from partly automating this task. Image-based identification systems exist, but mostly rely on hand-crafted algorithms to extract sets of features chosen a priori to identify species of selected taxa. In consequence, such systems are restricted to these taxa and additionally require involving experts that provide taxonomical knowledge for developing such customized systems. The aim of this study was to develop a deep learning system to learn discriminative features from leaf images along with a classifier for species identification of plants. By comparing our results with customized systems like LeafSnap we can show that learning the features by a convolutional neural network (CNN) can provide better feature representation for leaf images compared to hand-crafted features. Methods We developed LeafNet, a CNN-based plant identification system. For evaluation, we utilized the publicly available LeafSnap, Flavia and Foliage datasets. Results Evaluating the recognition accuracies of LeafNet on the LeafSnap, Flavia and Foliage datasets reveals a better performance of LeafNet compared to hand-crafted customized systems. Conclusions Given the overall species diversity of plants, the goal of a complete automatisation of visual plant species identification is unlikely to be met solely by continually gathering assemblies of customized, specialized and hand-crafted (and therefore expensive) identification systems. Deep Learning CNN approaches offer a self-learning state-of-the-art alternative that allows adaption to different taxa just by presenting new training data instead of developing new software systems.},
  keywords = {Convolutional layers,Convolutional neural network,Deep learning,Feature maps,Feature representation,Plant classification}
}

@misc{Drechsler-SantosKarstedtEtAl.MINDFunga.2023,
  title = {{{MIND}}.{{Funga}}},
  shorttitle = {{{MIND}}.{{Funga}}},
  author = {{Drechsler-Santos}, Elisandro Ricardo and Karstedt, Fernanda and Zimmermann Loureiro Chaves, Thiago and Titton, Mahatm{\=a} and {Martins da Cunha}, Kelmer and Leopoldo, Elo{\'i}sa and {Alves-Silva}, Genivaldo and Rezende, Diogo and Kossmann, Thiago and Gumboski, Emerson Luiz and {von Wangenheim}, Aldo},
  year = {2023},
  month = jul,
  publisher = {Mendeley Data},
  doi = {10.17632/sfrbdjvxcc.2},
  urldate = {2024-06-05}
}

@article{FarleyMehrotaEtAl.ImprovingYourModel.2024,
  title = {Improving Your Model},
  author = {Farley, Patrick and Mehrota, Nitin and Urban, Eric},
  year = {2024},
  month = feb,
  journal = {Microsoft Azure},
  urldate = {2024-06-06}
}

@inproceedings{GallRazaviEtAl.IntroductionRandomForests.2012,
  title = {An {{Introduction}} to {{Random Forests}} for {{Multi-class Object Detection}}},
  booktitle = {Outdoor and {{Large-Scale Real-World Scene Analysis}}},
  author = {Gall, Juergen and Razavi, Nima and Van Gool, Luc},
  editor = {Dellaert, Frank and Frahm, Jan-Michael and Pollefeys, Marc and {Leal-Taix{\'e}}, Laura and Rosenhahn, Bodo},
  year = {2012},
  pages = {243--263},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  abstract = {Object detection in large-scale real-world scenes requires efficient multi-class detection approaches. Random forests have been shown to handle large training datasets and many classes for object detection efficiently. The most prominent example is the commercial application of random forests for gaming [37]. In this paper, we describe the general framework of random forests for multi-class object detection in images and give an overview of recent developments and implementation details that are relevant for practitioners.},
  isbn = {978-3-642-34091-8}
}

@inproceedings{H.ZhaoF.GeEtAl.IdentificationWildMushroom.2021,
  title = {Identification of {{Wild Mushroom Based}} on {{Ensemble Learning}}},
  booktitle = {2021 {{IEEE}} 4th {{International Conference}} on {{Big Data}} and {{Artificial Intelligence}} ({{BDAI}})},
  author = {{H. Zhao} and {F. Ge} and {P. Yu} and {H. Li}},
  year = {2021},
  pages = {43--47},
  doi = {10.1109/BDAI52447.2021.9515225}
}

@article{HollisterCaiEtAl.UsingComputerVision.2023,
  title = {Using Computer Vision to Identify Limpets from Their Shells: A Case Study Using Four Species from the {{Baja California}} Peninsula},
  author = {Hollister, Jack D. and Cai, Xiaohao and Horton, Tammy and Price, Benjamin W. and Zarzyczny, Karolina M. and Fenberg, Phillip B.},
  year = {2023},
  journal = {Frontiers in Marine Science},
  volume = {10},
  issn = {2296-7745},
  abstract = {The shell morphology of limpets can be cryptic and highly variable, within and between species. Therefore, the visual identification of species can be troublesome even for experts. Here, we demonstrate the capability of computer vision models as a new method to assist with identifications. We investigate the ability of computers to distinguish between four species and two genera of limpets from the Baja California peninsula (Mexico) from digital images of shells from both dorsal and ventral orientations. Overall, the models performed marginally better (97.9\%) than experts (97.5\%) when predicting the same set of images and did so 240x faster. Moreover, we utilised a heatmap system to both verify that models are focussing on the specimens and to view which features on the specimens the models used to distinguish between species and genera. We then enlisted the expertise of limpet ecologists specialised in identification of species from the Baja peninsula to comment on whether the heatmaps are indeed focusing on specific morphological features per species/genus. They confirm that in their opinion, the majority of the heatmaps appear to be highlighting areas and features of morphological importance for distinguishing between groups. Our findings reveal that the cutting-edge technology of computer vision holds tremendous potential in enhancing species identification techniques used by taxonomists and ecologists. Not only does it provide a complementary approach to traditional methods, but it also opens new avenues for exploring the biology and ecology of limpets in greater detail.}
}

@book{JamesWittenEtAl.IntroductionStatisticalLearning.2023,
  title = {An {{Introduction}} to {{Statistical Learning}} with {{Applications}} in {{Python}}},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert and Taylor, Jonathan},
  year = {2023},
  publisher = {Springer International Publishing}
}

@article{LuckingAimeEtAl.UnambiguousIdentificationFungi.2020,
  title = {Unambiguous Identification of Fungi: Where Do We Stand and How Accurate and Precise Is Fungal {{DNA}} Barcoding?},
  author = {L{\"u}cking, Robert and Aime, M. Catherine and Robbertse, Barbara and Miller, Andrew N. and Ariyawansa, Hiran A. and Aoki, Takayuki and Cardinali, Gianluigi and Crous, Pedro W. and Druzhinina, Irina S. and Geiser, David M. and Hawksworth, David L. and Hyde, Kevin D. and Irinyi, Laszlo and Jeewon, Rajesh and Johnston, Peter R. and Kirk, Paul M. and Malosso, Elaine and May, Tom W. and Meyer, Wieland and {\"O}pik, Maarja and Robert, Vincent and Stadler, Marc and Thines, Marco and Vu, Duong and Yurkov, Andrey M. and Zhang, Ning and Schoch, Conrad L.},
  year = {2020},
  month = jul,
  journal = {IMA Fungus},
  volume = {11},
  number = {1},
  pages = {14},
  issn = {2210-6359},
  doi = {10.1186/s43008-020-00033-z},
  abstract = {True fungi (Fungi) and fungus-like organisms (e.g. Mycetozoa, Oomycota) constitute the second largest group of organisms based on global richness estimates, with around 3 million predicted species. Compared to plants and animals, fungi have simple body plans with often morphologically and ecologically obscure structures. This poses challenges for accurate and precise identifications. Here we provide a conceptual framework for the identification of fungi, encouraging the approach of integrative (polyphasic) taxonomy for species delimitation, i.e. the combination of genealogy (phylogeny), phenotype (including autecology), and reproductive biology (when feasible). This allows objective evaluation of diagnostic characters, either phenotypic or molecular or both. Verification of identifications is crucial but often neglected. Because of clade-specific evolutionary histories, there is currently no single tool for the identification of fungi, although DNA barcoding using the internal transcribed spacer (ITS) remains a first diagnosis, particularly in metabarcoding studies. Secondary DNA barcodes are increasingly implemented for groups where ITS does not provide sufficient precision. Issues of pairwise sequence similarity-based identifications and OTU clustering are discussed, and multiple sequence alignment-based phylogenetic approaches with subsequent verification are recommended as more accurate alternatives. In metabarcoding approaches, the trade-off between speed and accuracy and precision of molecular identifications must be carefully considered. Intragenomic variation of the ITS and other barcoding markers should be properly documented, as phylotype diversity is not necessarily a proxy of species richness. Important strategies to improve molecular identification of fungi are: (1) broadly document intraspecific and intragenomic variation of barcoding markers; (2) substantially expand sequence repositories, focusing on undersampled clades and missing taxa; (3) improve curation of sequence labels in primary repositories and substantially increase the number of sequences based on verified material; (4) link sequence data to digital information of voucher specimens including imagery. In parallel, technological improvements to genome sequencing offer promising alternatives to DNA barcoding in the future. Despite the prevalence of DNA-based fungal taxonomy, phenotype-based approaches remain an important strategy to catalog the global diversity of fungi and establish initial species hypotheses}
}

@article{LuLiawEtAl.DevelopmentMushroomGrowth.2019,
  title = {Development of a {{Mushroom Growth Measurement System Applying Deep Learning}} for {{Image Recognition}}},
  author = {Lu, Chuan-Pin and Liaw, Jiun-Jian and Wu, Tzu-Ching and Hung, Tsung-Fu},
  year = {2019},
  journal = {Agronomy},
  volume = {9},
  number = {1},
  issn = {2073-4395},
  doi = {10.3390/agronomy9010032},
  abstract = {In Taiwan, mushrooms are an agricultural product with high nutritional value and economic benefit. However, global warming and climate change have affected plant quality. As a result, technological greenhouses are replacing traditional tin houses as locations for mushroom planting. These greenhouses feature several complex parameters. If we can reduce the complexity such greenhouses and improve the efficiency of their production management using intelligent schemes, technological greenhouses could become the expert assistants of farmers. In this paper, the main goal of the developed system is to measure the mushroom size and to count the amount of mushrooms. According to the results of each measurement, the growth rate of the mushrooms can be estimated. The proposed system also records the data of the mushrooms and broadcasts them to the mobile phone of the farmer. This improves the effectiveness of the production management. The proposed system is based on the convolutional neural network of deep learning, which is used to localize the mushrooms in the image. A positioning correction method is also proposed to modify the localization result. The experiments show that the proposed system has a good performance concerning the image measurement of mushrooms.},
  keywords = {artificial intelligence,computer vision,convolutional neural network,deep learning,image measurement,mushroom cultivation}
}

@article{RahmanFaruqEtAl.IoTEnabledMushroom.2022,
  title = {{{IoT}} Enabled Mushroom Farm Automation with {{Machine Learning}} to Classify Toxic Mushrooms in {{Bangladesh}}},
  author = {Rahman, Hasibur and Faruq, Md. Omar and Abdul Hai, Talha Bin and Rahman, Wahidur and Hossain, Muhammad Minoar and Hasan, Mahbubul and Islam, Shafiqul and Moinuddin, {\relax Md}. and Islam, Md. Tarequl and Azad, Mir Mohammad},
  year = {2022},
  month = mar,
  journal = {Journal of Agriculture and Food Research},
  volume = {7},
  pages = {100267},
  issn = {2666-1543},
  doi = {10.1016/j.jafr.2021.100267},
  abstract = {The recent statistics on Agriculture identically remarks the massive contributions of mushroom farming in the Global market. Thus, the popularity of mushroom farming and cultivation is increasing day after day. Weather monitoring and management is a significant feature for mushroom growth, especially the impact of temperature and humidity. Most of the farmers in remote areas worldwide use traditional ways to cultivate the mushrooms. The traditional way is very complicated, and often poisonous mushrooms appear due to the lack of sufficient weather and cultivation process monitoring. This paper reflects an architectural design of IoT \& Machine Learning (ML)-based Smart Mushroom Farming. The proposed system introduces Remote Monitoring and Management (RMM), Farm Automation, and Mushroom classification. The Internet of Things (IoT), the microcontroller ESP32, and some agriculture-related sensors enable smart monitoring and farm automation. Machine Learning (ML) technology has been adopted to classify edible mushrooms to avoid poisonous mushrooms. To investigate the efficiency of the proposed system, several experiments have been enumerated and interpreted. The study is further furnished with the System Usability Scale (SUS) to track the regular user's satisfaction and gained the SUS score of 82\%. The ML model is utilized by an ensemble model that is composed of six classifiers namely Decision Tree (DT), Logistic Regression (LR), K-nearest neighbor (KNN), Support Vector Machine (SVM), Na{\"i}ve Bayes (NB), and Random Forest (RF). The highest accuracy gained with the ensemble model is 100\% which outperforms each individual classifier. However, the system will be efficient for real-time farm automation and cultivation in the mushroom industry.},
  keywords = {Automation,ESP32,Internet of Things (IoT),Machine Learning (ML),Remote Monitoring and Management (RMM)}
}

@article{S.RenK.HeEtAl.FasterRCNNRealTime.2017,
  title = {Faster {{R-CNN}}: {{Towards Real-Time Object Detection}} with {{Region Proposal Networks}}},
  author = {{S. Ren} and {K. He} and {R. Girshick} and {J. Sun}},
  year = {2017},
  month = jun,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {39},
  number = {6},
  pages = {1137--1149},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2016.2577031}
}

@article{SchneiderTaylorEtAl.PresentFutureApproaches.2019,
  title = {Past, Present and Future Approaches Using Computer Vision for Animal Re-Identification from Camera Trap Data},
  author = {Schneider, Stefan and Taylor, Graham W. and Linquist, Stefan and Kremer, Stefan C.},
  year = {2019},
  month = apr,
  journal = {Methods in Ecology and Evolution},
  volume = {10},
  number = {4},
  pages = {461--470},
  publisher = {John Wiley \& Sons, Ltd},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.13133},
  urldate = {2024-06-05},
  abstract = {Abstract The ability of a researcher to re-identify (re-ID) an individual animal upon re-encounter is fundamental for addressing a broad range of questions in the study of ecosystem function, community and population dynamics and behavioural ecology. Tagging animals during mark and recapture studies is the most common method for reliable animal re-ID; however, camera traps are a desirable alternative, requiring less labour, much less intrusion and prolonged and continuous monitoring into an environment. Despite these advantages, the analyses of camera traps and video for re-ID by humans are criticized for their biases related to human judgement and inconsistencies between analyses. In this review, we describe a brief history of camera traps for re-ID, present a collection of computer vision feature engineering methodologies previously used for animal re-ID, provide an introduction to the underlying mechanisms of deep learning relevant to animal re-ID, highlight the success of deep learning methods for human re-ID, describe the few ecological studies currently utilizing deep learning for camera trap analyses and our predictions for near future methodologies based on the rapid development of deep learning methods. For decades, ecologists with expertise in computer vision have successfully utilized feature engineering to extract meaningful features from camera trap images to improve the statistical rigor of individual comparisons and remove human bias from their camera trap analyses. Recent years have witnessed the emergence of deep learning systems which have demonstrated the accurate re-ID of humans based on image and video data with near perfect accuracy. Despite this success, ecologists have yet to utilize these approaches for animal re-ID. By utilizing novel deep learning methods for object detection and similarity comparisons, ecologists can extract animals from an image/video data and train deep learning classifiers to re-ID animal individuals beyond the capabilities of a human observer. This methodology will allow ecologists with camera/video trap data to reidentify individuals that exit and re-enter the camera frame. Our expectation is that this is just the beginning of a major trend that could stand to revolutionize the analysis of camera trap data and, ultimately, our approach to animal ecology.},
  keywords = {animal reidentification,camera traps,computer vision,convolutional networks,deep learning,density estimation,monitoring,object detection}
}

@article{Shafl.RandomForestClassification.2023,
  title = {Random {{Forest Classification}} with {{Scikit-Learn}}},
  author = {Shafl, Adam},
  year = {2023},
  journal = {Datacamp}
}

@article{Shepard.LatestComputerVision.2022,
  title = {The {{Latest Computer Vision Model Updates}}},
  author = {Shepard, Alex},
  year = {2022},
  month = apr,
  journal = {iNaturalist},
  urldate = {2024-06-06}
}

@article{WangZhengEtAl.AutomaticSortingSystem.2018,
  title = {An Automatic Sorting System for Fresh White Button Mushrooms Based on Image Processing},
  author = {Wang, Fengyun and Zheng, Jiye and Tian, Xincheng and Wang, Jianfei and Niu, Luyan and Feng, Wenjie},
  year = {2018},
  month = aug,
  journal = {Computers and Electronics in Agriculture},
  volume = {151},
  pages = {416--425},
  issn = {0168-1699},
  doi = {10.1016/j.compag.2018.06.022},
  abstract = {White button mushrooms, one of the most commonly and widely consumed mushrooms worldwide, require grading before entering the market. The diameter of the pileus is an important factor that affects the selling price. The aim of this study is to develop an automatic sorting system for fresh white button mushrooms based on image processing according to the pileus diameter. First, an automatic sorting hardware system is designed that consists of a conveying mechanism, an image acquisition system, a control module and an actuator. Second, we present an image algorithm based on the watershed method, Canny operator, OR operation, and closed operation to determine the pileus diameter of fresh white button mushrooms. The algorithm eliminates the influence of the shadow and petiole on the image. Third, a precise control strategy is introduced based on the conveyor speed, distance between the trigger and the flap piece, trigger time and algorithm processing time. Finally, we present analysis and control software based on OpenCV 2.4.10 and Visual Studio 2010. In the experimental portion, a prototype system was tested to validate its applicability and reliability by the grading performance and results. The experimental results showed that the average maximum grading speed was 102.41\,mushrooms/minute, the accuracy of grading was 97.42\%, the damage rate was 0.05\%, and the undetected rate was 0.96\%. Relative to manual grading, the grading speed was improved by 38.86\%, and the accuracy was improved by 6.84\%. The system can operate stably and continuously. For long-duration grading in particular, the advantage of the automatic sorting system is apparent because it eliminates labourer fatigue. The system achieves online automatic sorting and grading of fresh white button mushrooms with minimal destruction.},
  keywords = {Automatic control,Image processing,Online sorting,White button mushroom}
}
