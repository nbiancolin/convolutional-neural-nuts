% !TEX root = project_proposal.tex
%
%  PACKAGE IMPORTS
%
\documentclass{article} % For LaTeX2e
\usepackage{iclr2022_conference,times}
\input{../math_commands.tex}

\newcommand{\apsname}{Progress Report}
\newcommand{\gpnumber}{26}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage[table]{xcolor}

\usepackage{graphicx} % Required for \resizebox
\usepackage{placeins} % Required for \FloatBarrier

%
%  TITLE AND AUTHORS
%
\title{Deep learning approach to  \\ 
mushroom species classification}

\author{Yanni Alan Alevras  \\
Student\# 1009330706 \\
\texttt{yanni.alevras@mail.utoronto.ca} \\
\And
Nicholas Biancolin  \\
Student\# 1009197726 \\
\texttt{n.biancolin@mail.utoronto.ca} \\
\AND
Eric Liu  \\
Student\# 1009098450 \\
\texttt{ey.liu@mail.utoronto.ca} \\
\And
Jason Ruixuan Zhang \\
Student\# 1008997631 \\
\texttt{jasonrx.zhang@mail.utoronto.ca} \\
\AND
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy

%
%   DOCUMENT STARTS HERE
%

\begin{document}
\maketitle

\section{Project Description}
\label{sec:project_description}

Fungi identification is an increasingly critical task, with implications in food security, industrial use, conservation efforts, and biosafety. However, visual and image classification of fungi is a difficult task due to the wide variety of species. The goal of this project is to develop a deep learning model that can accurately identify macrofungi (fungi with large bodies) based on their genus. 

87 percent of the land in Ontario is Crown land, filled with expansive forests and a diverse array of plant species. Among these, mushrooms stand out as both common and challenging to identify due to their wide variety and the subtle differences between species.This identification task is particularly crucial because while some mushrooms are edible, others are highly poisonous, potentially posing a significant risk to foragers and nature enthusiasts.

We believe deep learning is a suitable approach for this task as deep learning models have proven to be effective in image classification tasks. Furthermore, there is a robust amount of data available for training, helping to ensure a solid model. We have selected the MIND.Funga dataset \citep{Drechsler-SantosKarstedtEtAl.MINDFunga.2023}, which has approximately 17 000 images of nearly 500 species of fungi. This dataset is well-suited for our project, as it is built primarily for use in deep classification models.


\section{Indvidual Contributions and Responsibilities}
\label{sec:individual_contributions_and_responsibilities}

All the members of our team have been friends for a long time, and we have worked together on various projects in the past. We have a good understanding of each other's strengths and weaknesses, and we have developed strategies to work together effectively. We have divided the work based on our individual strengths and interests, and we have regular meetings to discuss our progress and make decisions together. We have no need for a formal project management system, however we track our progress using Git, and we demo our work to each other regularly.

\textbf{do we need this??}

\subsection{Yanni Alevras}
\label{subsec:yanni_alevras}

\textbf{Work Completed to date:}
Researching which forms of data augmentation are most relevant to the project. Data augmentation additions to the dataset. Data Processing and Individual Contributions and Responsibilities sections of the report. 

\textbf{Work to be completed:}

\subsection{Nicholas Biancolin}
\label{subsec:nicholas_biancolin}

\textbf{Work Completed to date:}
Training and preliminary tuning of model. Project Description section of report. Individual Contributions and Responsibilities
\textbf{Work to be completed:}

\subsection{Eric Liu}
\label{subsec:eric_liu}

\textbf{Work Completed to date:}
Rough Draft for Primary Model section of the report. handdrawn diagram of structure. Data Separation and Apply Transfer Learning: alexNet. Class Structure, Training Function, Accuracy function. Validation Loss and Graphs.

\textbf{Work to be completed:}

\subsection{Jason Zhang}
\label{subsec:jason_zhang}

\textbf{Work Completed to date:}
Set up python environment for the group to work with. Genus grouping to reduce size of dataset. Random forest baseline model with accuracy, precision, recall, F1, and support reports. Baseline Model section of the report..

\textbf{Work to be completed:}

\begin{table}[ht]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{|l|c|c|c|c|}
        \hline
        \rowcolor{gray!50}
        \textbf{Project Progress Report} & \textbf{Eric Liu} & \textbf{Jason Zhang} & \textbf{Nicholas Biancolin} & \textbf{Yanni Alevras} \\
        \hline
        \rowcolor{green!50}
        Brief Project Description (June 30th, 11:59 pm) & & & W & \\
        \hline
        \rowcolor{green!50}
        Individual Contributions and Responsibilities (June 30th, 11:59 pm) & & & W & W \\
        \hline
        \rowcolor{green!50}
        Contributions - Data Processing (June 30th, 11:59 pm) & & & & W \\
        \hline
        \rowcolor{green!50}
        Contributions - Baseline Model (June 30th, 11:59 pm) & & W & & \\
        \hline
        \rowcolor{green!50}
        Contributions - Primary Model (June 30th, 11:59 pm) & W & & & \\
        \hline
        \rowcolor{green!50}
        Illustrations (July 1st, 11:59 pm) & W & & & W \\
        \hline
        \rowcolor{green!50}
        Latex format (July 2nd, 11:59 pm) & W & W & W & W \\
        \hline
        \rowcolor{green!50}
        Editing (July 3rd, 11:59 pm) & ED & ED & ED & ED\\
        \hline
        \rowcolor{green!50}
        Final Proofread (July 4th, 6:00 pm) & W & W & W & W \\
        \hline
    \end{tabular}}
    \caption{Project Progress Report Task Breakdown}
\end{table}

\FloatBarrier

\begin{table}[ht]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{|l|c|c|c|c|}
        \hline
        \rowcolor{gray!50}
        \textbf{Project - Training and Testing} & \textbf{Eric Liu} & \textbf{Jason Zhang} & \textbf{Nicholas Biancolin} & \textbf{Yanni Alevras} \\
        \hline
        \rowcolor{green!50}
        Data Cleaning (June 16th, 11:59pm) & & W & & W \\
        \hline
        \rowcolor{green!50}
        Image Grouping (June 16th, 11:59pm) & & W & & ED \\
        \hline
        \rowcolor{green!50}
        Transfer data to training format (June 16th, 11:59pm) & W & & & \\
        \hline
        \rowcolor{green!50}
        Data annotations, splitting (June 16th, 11:59pm) & W & W & & ED \\
        \hline
        \rowcolor{green!50}
        Data Augmentation (June 16th, 11:59pm) & & & & W \\
        \hline
        \rowcolor{green!50}
        Data annotations, splitting (June 16th, 11:59pm) & W & & & ED \\
        \hline
        \rowcolor{green!50}
        Transfer Learning Connection (June 19th, 11:59pm) & W & & & \\
        \hline
        \rowcolor{green!50}
        CNN architecture (June 19th, 11:59pm) & W & ED & &  \\
        \hline
        \rowcolor{green!50}
        Training Loop (June 19th, 11:59pm) & W & ED & & \\
        \hline
        \rowcolor{green!50}
        Plotting and Accuracy (June 19th, 11:59pm) & W & ED & W & \\
        \hline
        \rowcolor{red!50}
        Hyperparameter adjustments (July 15th, 11:59pm) & W & ED & & W \\
        \hline
        \rowcolor{red!50}
        Iterative (if needed) (August 10th, 11:59pm) & & ED & & W \\
        \hline
        \rowcolor{red!50}
        Evaluation (August 10th, 11:59pm) & W & W & W ED & W\\
        \hline
        \rowcolor{red!50}
        Documentation (August 10th, 11:59pm) & W & W & ED & \\
        \hline
        \rowcolor{red!50}
        Resource management (August 3rd, 11:59pm) & & & W & ED \\
        \hline
        \rowcolor{red!50}
        Analyze Result for Presentation and Project (August 3rd, 11:59pm) & W & W & W & W \\
        \hline
    \end{tabular}}
    \caption{Project Training and Testing Task Breakdown}
\end{table}

\FloatBarrier

\begin{table}[ht]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{|l|c|c|c|c|}
        \hline
        \rowcolor{gray!50}
        \textbf{Presentation} & \textbf{Eric Liu} & \textbf{Jason Zhang} & \textbf{Nicholas Biancolin} & \textbf{Yanni Alevras} \\
        \hline
        \rowcolor{red!50}
        Presentation Brainstorm (August 5th, 11:59pm) & W & W & W & W \\
        \hline
        \rowcolor{red!50}
        Problem - slides (August 5th, 11:59pm) & & ED & W & \\
        \hline
        \rowcolor{red!50}
        Data Processing - slides (August 5th, 11:59pm) & W & & & ED \\
        \hline
        \rowcolor{red!50}
        Model - slides (August 5th, 11:59pm) & & W & ED & \\
        \hline
        \rowcolor{red!50}
        Result - slides (August 5th, 11:59pm) & ED & & & W \\
        \hline
        \rowcolor{red!50}
        Slides Editing (August 5th, 11:59pm) & ED & ED & & \\
        \hline
        \rowcolor{red!50}
        Individual Practice (August 7th, 11:59pm) & W & W & W & W\\
        \hline
        \rowcolor{red!50}
        Group Practice (August 7th, 11:59pm) & W & W & W & W \\
        \hline
        \rowcolor{red!50}
        Record Presentation (August 7th, 11:59pm) & W & W & W & W \\
        \hline
        \rowcolor{red!50}
        Editing (August 10th, 11:59pm) & & & W & ED \\
        \hline
    \end{tabular}}
    \caption{Presentation Task Breakdown}
\end{table}

\FloatBarrier

\begin{table}[ht]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{|l|c|c|c|c|}
        \hline
        \rowcolor{gray!50}
        \textbf{Project Final Report} & \textbf{Eric Liu} & \textbf{Jason Zhang} & \textbf{Nicholas Biancolin} & \textbf{Yanni Alevras} \\
        \hline
        \rowcolor{red!50}
        Latex Formatting (August 12th, 11:59pm) & W & W & ED & \\
        \hline
        \rowcolor{red!50}
        Introduction (August 7th, 11:59pm) & & ED & W & W\\
        \hline
        \rowcolor{red!50}
        Illustration (August 7th, 11:59pm) & W & ED & & \\
        \hline
        \rowcolor{red!50}
        Background and Related Work (August 7th, 11:59pm) & & W & ED & ED \\
        \hline
        \rowcolor{red!50}
        Data Processing (August 7th, 11:59pm) & & W & & ED \\
        \hline
        \rowcolor{red!50}
        Architecture (August 7th, 11:59pm) & ED & & W & \\
        \hline
        \rowcolor{red!50}
        Baseline Model (August 7th, 11:59pm) & ED & ED & & W \\
        \hline
        \rowcolor{red!50}
        Qualitative Results (August 7th, 11:59pm) & W & ED & & \\
        \hline
        \rowcolor{red!50}
        Quantitative Results (August 7th, 11:59pm) & & W & & ED \\
        \hline
        \rowcolor{red!50}
        Evaluation of Model (August 7th, 11:59pm) & W & & W ED& ED\\
        \hline
        \rowcolor{red!50}
        Discussion (August 7th, 11:59pm) & ED & W & & W\\
        \hline
        \rowcolor{red!50}
        Ethical Considerations (August 7th, 11:59pm) & & W& & ED \\
        \hline
        \rowcolor{red!50}
        Project Difficulty (August 7th, 11:59pm) & W & ED & & \\
        \hline
        \rowcolor{red!50}
        Editing (August 12th, 11:59pm) & ED & & ED & W \\
        \hline
        \rowcolor{red!50}
        Final Proofread (August 14th, 11:59pm) & W & W & W & W\\
        \hline
    \end{tabular}}
    \caption{Project Final Report Task Breakdown}
\end{table}

\FloatBarrier

\section{Notable Contribution}
\label{sec:notable_contribution}

\subsection{Data Processing}

<<<<<<< HEAD
The dataset contains 509 species. Stated in our project proposal, due to some low amount of sample imaging for some species we decided to group by genus, and keep the top 15 genera with the largest datasets. This genus grouping was done through iterating all of the species in the dataset, and moving all of the data into a new directory for that genus. These directories were then ordered by size, and the top 15 were kept.


To artificially increase the amount of data in our dataset, we used data augmentation, creating a copy of each sample with a horizontal flip, 90 degree rotation, 180 degree rotation, 270 degree rotation, gaussian noise, and random erasing (small black rectangles). These methods were preferred over others such as kernel filters, lowering the quality of an image. Since these mushrooms can be identified based on specific visual traits found within their genus, high detail in the training images are necessary to allow for differentiation between the different genera. Due to this, prioritizing the quality of the image was necessary. In addition, our model is colour agnostic, so having a method like color space transformations are not necessary, and would allow for too many almost duplicate images in the dataset. Some simple methods were the flip and rotations, which kept the same image, but just made the model look at it a different way. Gaussian noise was added as a technique to allow the model to draw attention towards the most robust features of the image, relevant to the nature of this dataset where many genera are similar in shape and very based on texture or pattern. Random erasing is important for training with data that varies in resolution. This dataset has mostly high quality images, but some are lower resolution. This method was added for a similar reason as gaussian noise, but instead to train the model to identify more distinct features of one genus to another (Shorten et al., 2019).

\FloatBarrier

\begin{figure}[h]
    \begin{center}
    \includegraphics[width=0.9\textwidth]{figures/data_aug.jpg}
    \end{center}
    \caption{Data Augmentation Examples}
\end{figure}

\FloatBarrier

To test this data, it is split into #\% training, #\% validation and #\% testing. Our evaluation metrics consist of accuracy, precision, recall, F1 and support. This is to provide a wider view on the performance of the model, including underrepresented classes.

The biggest challenge for the data processing, was figuring out how much original data we would need, and how much we want to use data processing to increase the amount of data. Including what types of data augmentation we would see fit, explained above. Overall the top 15 genera have around 200-1000 images each, with data augmentation multiplying that my seven.




Primary Model


\subsection{Baseline Model}
Our baseline model is a random forests classifier trained with scikit-learn. As mentioned in our proposal, decision trees like random forest are generally well-suited for multiclass problems \citep{GallRazaviEtAl.IntroductionRandomForests.2012}.

We follow similar processing steps as the primary model, in the following order: a test-train split of 80\%/20\%, feature extraction with HOG, training, and performance assessments. The model uses the same dataset as the primary model with the exclusion of augmented data and additional restrictions on image size (300 x 300) and number of colour channels. The restriction on image size is primarily to reduce training complexity and time.

A conversion to grayscale is necessary to facilitate feature extraction with histogram of oriented gradients (HOG) \citep{AbouelnagaThambirajaEtAl.ObjectDetectionHistogram.2018}, computed with scikit-image. As models like random forest do not automatically perform feature extraction (in comparison to deep convolutional networks, which our primary model uses), this step is done to provide the model with a set of features to learn from. We compute HOG features on both the training and testing set, in line with the process outlined by \cite{Shafi.RandomForestClassification.2023}. We use default hyperparameters for training, with the exception of the number of estimators, which is set to 500, which \cite{Xi.ImageClassificationRecognition.2022} suggests is an optimal amount of trees for increasing accuracy.

Training takes approximately 4 minutes on a CPU. This is significantly faster than the primary model, which takes % get this number

The model has a training accuracy of about 54\%, roughly 10\% worse than the primary model. This is expected, as random forests are generally less accurate than deep learning models for image classification tasks. We also used minimal hyperparameter tuning, which could have otherwise improved the model's performance. A more complicated architecture, like an ensemble model with convolutional feature extraction could have also improved model performance \citep{Xi.ImageClassificationRecognition.2022}.

\begin{figure}[h]
    \begin{center}
    \includegraphics[width=1.0\textwidth]{figures/baseline.png}
    \end{center}
    \caption{Precision, recall, and F1-score for the baseline model}
\end{figure}

The precision, recall, and F1-score are plotted in the following figure. We have an average precision of 0.67, recall of 0.44, and F-score of 0.47. Notably, the recall and F-score data take a similar shape, which lower-support classes on average performing worse in recall and F-score compared to higher-support classes, likely due to less data to learn from.

Our highest precision is 1.0 with Schizophyllum, a class with a low support number. Notably its corresponding recall is 0.08, suggesting a high number of false negatives. This trend follows with other low-support classes. Our highest recall is 0.85 with Marasmius, the highest support class. A larger support is generally correlated with a higher recall.

Notably, the model disproportionately predicts more false positives of Hygrocybe than any other class, as reflected in the confusion matrix's predicted labels. This may suggest feature similarity between Hygrocybe and other classes, or a lack of distinguishing features in the training data.

We faced several issues with training. We used Intel's x86-64 architecture-specific optimizations to reduce training time \citep{SchlimbachAndreevEtAl.IntelExtensionScikitlearn.2023}, and on a few instances had memory allocation issues during training.

\begin{figure}[h]
    \begin{center}
    \includegraphics[width=0.6\textwidth]{figures/baseline_matrix.png}
    \end{center}
    \caption{Random forest confusion matrix}
\end{figure}

\subsection{Primary Model}
\begin{figure}[h]
    \begin{center}
    \includegraphics[width=0.9\textwidth]{figures/primaryModelDiagram.png}
    \end{center}
    \caption{Model Structure and Tensor Sizes}
\end{figure}


Our CNN model consists of two main sections: a non-tunable transfer learning part and a tunable convolution and fully connected layers section.

\subsubsection{Non-Tunable Section}
  
The team uses AlexNet for its high-level feature extraction. AlexNet processes a $3 \times 244 \times 244$ input image and the feature extraction outputs a $256 \times 6 \times 6$ tensor \citep{Bangar.AlexNetArchitectureExplained.2022}. There are five convolutional layers and three pooling layers \citep{Bangar.AlexNetArchitectureExplained.2022}, the order of the layers is shown on the hand-drawn diagram above. Since the model needs to differentiate between mushrooms with very similar appearances, AlexNet excels in extracting the fine features that set them apart.
  
\subsubsection{Tunable Section}
  
To make the model specific to the team’s project, the team uses one additional convolutional layer, outputting a $128 \times 6 \times 6$ tensor. After the additional convolutional layer, the output gets flattened and passed through three fully connected layers with ReLU activation functions in between. The fully connected layers turned the size from 4608 to 256, then to 128, and lastly to 30, matching the number of output classes the team decided for the model.
  

In total, there are $5+3$ layers in the non-tunable section and $1+3$ layers in the tunable section, making our class structure 12 layers in total.
  
\begin{figure}[h]
    \begin{center}
    \includegraphics[width=0.6\textwidth]{figures/AlexNetStructure.png}
    \end{center}
    \caption{Class Structure: AlexNet \citep{Bangar.AlexNetArchitectureExplained.2022}}
\end{figure}

\subsubsection{Calculation of Parameters}

\subsubsubsection{Number of parameters for the AlexNet structure:}
\begin{align*}
\text{Conv1} & = 3 \times 11 \times 11 \times (96 + 1) = 35,271 \\
\text{Conv2} & = 96 \times 5 \times 5 \times (256 + 1) = 616,800 \\
\text{Conv3} & = 256 \times 3 \times 3 \times (384 + 1) = 886,080 \\
\text{Conv4} & = 384 \times 3 \times 3 \times (384 + 1) = 1,310,720 \\
\text{Conv5} & = 384 \times 3 \times 3 \times (256 + 1) = 887,232 \\
\end{align*}

\subsubsection{Number of parameters for the tunable section:}
\begin{align*}
\text{Conv1} & = 256 \times 3 \times 3 \times (128 + 1) = 297,216 \\
\text{Fc1} & = 4608 \times (256 + 1) = 1,183,296 \\
\text{Fc2} & = 256 \times (128 + 1) = 33,024 \\
\text{Fc3} & = 128 \times (30 + 1) = 3,968 \\
\end{align*}

The total number of parameters is 5,273,927, the number of trainable parameters is only 1,517,504. This ensures the training time for our models is feasible, allowing the team to focus on more epochs and more variations using data augmentations in the future.

At the start, the team pushed all images into the feature extraction part of AlexNet, converting data into tensors. We randomly split the data into a 75\%, 15\%, and 10\% ratio for training, validation, and testing.

For our current best result, we used a batch size of 36, learning rate of 0.007, and 15 epochs. We chose Cross Entropy Loss for the loss function as we want the model to classify the image into one of the 30 classes. For the optimizer, the group decided on Stochastic Gradient Descent (SGD).

\begin{figure}[h]
    \begin{center}
    \includegraphics[width=0.6\textwidth]{figures/validationLossGraph.png}
    \end{center}
    \caption{Validation Loss}
\end{figure}

The team's training graph with the specified hyper-parameters is shown above. It indicates that the model is overfitted quickly, to tackle this issue, the team aims to implement regularization and drop off in the future.

\textbf{Testing Accuracy:}
\begin{itemize}
    \item Epoch 4: Test Classification Accuracy: 64.01\%
    \item Epoch 8: Test Classification Accuracy: 65.12\%
\end{itemize}

From the above graph, the team chose epochs 4 and epoch 8 and did accuracy testing, using the testing data. The testing accuracy is at a good starting point considering the model must classify an image into one of thirty classes. The accuracy can be improved using various techniques:
\begin{itemize}
    \item The current data used in the training is not augmented. The data augmentation functions are completed, but not used currently to save runtime for the training loop. The team will add the augmented data as part of training in the future.
    \item Further hyperparameters fine tuning.
    \item Implementing regularization and drop off to reduce the quick overfitting seen in the validation graph.
\end{itemize}


\label{last_page}

\bibliography{progress_ref}
\bibliographystyle{iclr2022_conference}

\end{document}
