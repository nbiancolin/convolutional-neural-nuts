% !TEX root = project_proposal.tex
%
%  PACKAGE IMPORTS
%
\documentclass{article} % For LaTeX2e
\usepackage{iclr2022_conference,times}
\input{../math_commands.tex}

\newcommand{\apsname}{Progress Report}
\newcommand{\gpnumber}{26}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage[table]{xcolor}

\usepackage{graphicx} % Required for \resizebox
\usepackage{placeins} % Required for \FloatBarrier

%
%  TITLE AND AUTHORS
%
\title{Deep learning approach to  \\ 
mushroom species classification}

\author{Yanni Alan Alevras  \\
Student\# 1009330706 \\
\texttt{yanni.alevras@mail.utoronto.ca} \\
\And
Nicholas Biancolin  \\
Student\# 1009197726 \\
\texttt{n.biancolin@mail.utoronto.ca} \\
\AND
Eric Liu  \\
Student\# 1009098450 \\
\texttt{ey.liu@mail.utoronto.ca} \\
\And
Jason Ruixuan Zhang \\
Student\# 1008997631 \\
\texttt{jasonrx.zhang@mail.utoronto.ca} \\
\AND
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy

%
%   DOCUMENT STARTS HERE
%

\begin{document}
\maketitle

\section{Project Description}
\label{sec:project_description}

Fungi identification is an increasingly critical task, with implications in food security, industrial use, conservation efforts, and biosafety. However, visual and image classification of fungi is a difficult task due to the wide variety of species. The goal of this project is to develop a deep learning model that can accurately identify macrofungi (fungi with large bodies) based on their genus. 

87 percent of the land in Ontario is Crown Land, filled with expansive forests and a diverse array of plant species. Among these, mushrooms stand out as both common and challenging to identify due to their wide variety and the subtle differences between species.This identification task is particularly crucial because while some mushrooms are edible, others are highly poisonous, potentially posing a significant risk to foragers and nature enthusiasts.

We believe deep learning is a suitable approach for this task as deep learning models have proven to be effective in image classification tasks. Furthermore, there is a robust amount of data available for training, helping to ensure a solid model.


\section{Indvidual Contributions and Responsibilities}
\label{sec:individual_contributions_and_responsibilities}

- How team is working together


-project management software used to communicate/track results


-detailed list of what everyone has worked on, and what they will be working on

Yanni:

For this milestone, Yanni contributed to the data augmentation portion of this project. Using 

Eric:

Rough Draft for Primary Model section of the report. handdrawn diagram of structure. Data Seperation and Apply Transfer Learning: alexNet. Class Structure, Training Function, Accuracy function. Validation Loss and Graphs.

\begin{center}
\begin{tabular}{ |c|c|c|c| }
\hline
\rowcolor{gray!50}
\textbf{Yanni} & \textbf{Nick} & \textbf{Eric} & \textbf{Jason} \\
\hline


\end{tabular}
\end{center}

\section{Notable Contribution}
\label{sec:notable_contribution}

\subsection{Data Processing}

The dataset contains \# species. Stated in our project proposal, due to some low amount of sample imaging for some species we decided to group by genus. Any genus with under \# samples would be removed. This narrowed us down from 509 species, and \# genus, to 15 genus. To artificially increase the amount of data in our dataset, we used data augmentation, creating a copy of each sample with a horizontal flip, 90 degree rotation, 180 degree rotation, 270 degree rotation, gaussian noise, and random erasing (small black rectangles). These methods were preferred over others such as kernel filters, lowering the quality of an image. Since these mushrooms can be identified based on specific visual traits found within their genus, high detail in the training images are necessary to allow for differentiation between the different genera. Due to this, prioritizing the quality of the image was necessary. Some simple methods were the flip and rotations, which kept the same image, but just made the model look at it a different way. Gaussian noise was added .................... Random erasing was added to

\subsection{Baseline Model}
Our baseline model is a random forests classifier trained with scikit-learn. As mentioned in our proposal, decision trees like random forest are generally well-suited for multiclass problems \citep{GallRazaviEtAl.IntroductionRandomForests.2012}.

We follow similar processing steps as the primary model, in the following order: a test-train split of 80\%/20\%, feature extraction with HOG, training, and performance assessments. The model uses the same dataset as the primary model with the exclusion of augmented data and additional restrictions on image size (300 x 300) and number of colour channels. The restriction on image size is primarily to reduce training complexity and time.

A conversion to grayscale is necessary to facilitate feature extraction with histogram of oriented gradients (HOG) \citep{AbouelnagaThambirajaEtAl.ObjectDetectionHistogram.2018}, computed with scikit-image. As models like random forest do not automatically perform feature extraction (in comparison to deep convolutional networks, which our primary model uses), this step is done to provide the model with a set of features to learn from. We compute HOG features on both the training and testing set, in line with the process outlined by \cite{Shafi.RandomForestClassification.2023}. We use default hyperparameters for training, with the exception of the number of estimators, which is set to 500, which \cite{Xi.ImageClassificationRecognition.2022} suggests is an optimal amount of trees for increasing accuracy.

Training takes approximately 4 minutes on a CPU. This is significantly faster than the primary model, which takes % get this number

The model has a training accuracy of about 54\%, roughly 10\% worse than the primary model. This is expected, as random forests are generally less accurate than deep learning models for image classification tasks. We also used minimal hyperparameter tuning, which could have otherwise improved the model's performance. A more complicated architecture, like an ensemble model with convolutional feature extraction could have also improved model performance \citep{Xi.ImageClassificationRecognition.2022}.

\begin{figure}[h]
    \begin{center}
    \includegraphics[width=1.0\textwidth]{figures/baseline.png}
    \end{center}
    \caption{Precision, recall, and F1-score for the baseline model}
\end{figure}

The precision, recall, and F1-score are plotted in the following figure. We have an average precision of 0.67, recall of 0.44, and F-score of 0.47. Notably, the recall and F-score data take a similar shape, which lower-support classes on average performing worse in recall and F-score compared to higher-support classes, likely due to less data to learn from.

Our highest precision is 1.0 with Schizophyllum, a class with a low support number. Notably its corresponding recall is 0.08, suggesting a high number of false negatives. This trend follows with other low-support classes. Our highest recall is 0.85 with Marasmius, the highest support class. A larger support is generally correlated with a higher recall.

Notably, the model disproportionately predicts more false positives of Hygrocybe than any other class, as reflected in the confusion matrix's predicted labels. This may suggest feature similarity between Hygrocybe and other classes, or a lack of distinguishing features in the training data.

We faced several issues with training. We used Intel's x86-64 architecture-specific optimizations to reduce training time \citep{SchlimbachAndreevEtAl.IntelExtensionScikitlearn.2023}, and on a few instances had memory allocation issues during training.

\begin{figure}[h]
    \begin{center}
    \includegraphics[width=0.6\textwidth]{figures/baseline_matrix.png}
    \end{center}
    \caption{Random forest confusion matrix}
\end{figure}

\subsection{Primary Model}
\begin{figure}[h]
    \begin{center}
    \includegraphics[width=0.9\textwidth]{figures/primaryModelDiagram.png}
    \end{center}
    \caption{Model Structure and Tensor Sizes}
\end{figure}


Our CNN model consists of two main sections: a non-tunable transfer learning part and a tunable convolution and fully connected layers section.

\subsubsection{Non-Tunable Section}
  
The team uses AlexNet for its high-level feature extraction. AlexNet processes a $3 \times 244 \times 244$ input image and the feature extraction outputs a $256 \times 6 \times 6$ tensor \citep{Bangar.AlexNetArchitectureExplained.2022}. There are five convolutional layers and three pooling layers \citep{Bangar.AlexNetArchitectureExplained.2022}, the order of the layers is shown on the hand-drawn diagram above. Since the model needs to differentiate between mushrooms with very similar appearances, AlexNet excels in extracting the fine features that set them apart.
  
\subsubsection{Tunable Section}
  
To make the model specific to the teamâ€™s project, the team uses one additional convolutional layer, outputting a $128 \times 6 \times 6$ tensor. After the additional convolutional layer, the output gets flattened and passed through three fully connected layers with ReLU activation functions in between. The fully connected layers turned the size from 4608 to 256, then to 128, and lastly to 30, matching the number of output classes the team decided for the model.
  

In total, there are $5+3$ layers in the non-tunable section and $1+3$ layers in the tunable section, making our class structure 12 layers in total.
  
\begin{figure}[h]
    \begin{center}
    \includegraphics[width=0.6\textwidth]{figures/AlexNetStructure.png}
    \end{center}
    \caption{Class Structure: AlexNet \citep{Bangar.AlexNetArchitectureExplained.2022}}
\end{figure}

\subsubsection{Calculation of Parameters}

\subsubsubsection{Number of parameters for the AlexNet structure:}
\begin{align*}
\text{Conv1} & = 3 \times 11 \times 11 \times (96 + 1) = 35,271 \\
\text{Conv2} & = 96 \times 5 \times 5 \times (256 + 1) = 616,800 \\
\text{Conv3} & = 256 \times 3 \times 3 \times (384 + 1) = 886,080 \\
\text{Conv4} & = 384 \times 3 \times 3 \times (384 + 1) = 1,310,720 \\
\text{Conv5} & = 384 \times 3 \times 3 \times (256 + 1) = 887,232 \\
\end{align*}

\subsubsection{Number of parameters for the tunable section:}
\begin{align*}
\text{Conv1} & = 256 \times 3 \times 3 \times (128 + 1) = 297,216 \\
\text{Fc1} & = 4608 \times (256 + 1) = 1,183,296 \\
\text{Fc2} & = 256 \times (128 + 1) = 33,024 \\
\text{Fc3} & = 128 \times (30 + 1) = 3,968 \\
\end{align*}

The total number of parameters is 5,273,927, the number of trainable parameters is only 1,517,504. This ensures the training time for our models is feasible, allowing the team to focus on more epochs and more variations using data augmentations in the future.

At the start, the team pushed all images into the feature extraction part of AlexNet, converting data into tensors. We randomly split the data into a 75\%, 15\%, and 10\% ratio for training, validation, and testing.

For our current best result, we used a batch size of 36, learning rate of 0.007, and 15 epochs. We chose Cross Entropy Loss for the loss function as we want the model to classify the image into one of the 30 classes. For the optimizer, the group decided on Stochastic Gradient Descent (SGD).

\begin{figure}[h]
    \begin{center}
    \includegraphics[width=0.6\textwidth]{figures/validationLossGraph.png}
    \end{center}
    \caption{Validation Loss}
\end{figure}

The team's training graph with the specified hyper-parameters is shown above. It indicates that the model is overfitted quickly, to tackle this issue, the team aims to implement regularization and drop off in the future.

\textbf{Testing Accuracy:}
\begin{itemize}
    \item Epoch 4: Test Classification Accuracy: 64.01\%
    \item Epoch 8: Test Classification Accuracy: 65.12\%
\end{itemize}

From the above graph, the team chose epochs 4 and epoch 8 and did accuracy testing, using the testing data. The testing accuracy is at a good starting point considering the model must classify an image into one of thirty classes. The accuracy can be improved using various techniques:
\begin{itemize}
    \item The current data used in the training is not augmented. The data augmentation functions are completed, but not used currently to save runtime for the training loop. The team will add the augmented data as part of training in the future.
    \item Further hyperparameters fine tuning.
    \item Implementing regularization and drop off to reduce the quick overfitting seen in the validation graph.
\end{itemize}


\label{last_page}

\bibliography{progress_ref}
\bibliographystyle{iclr2022_conference}

\end{document}
