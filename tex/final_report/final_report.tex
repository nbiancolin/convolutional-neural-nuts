% !TEX root = final_report.tex
%
%  PACKAGE IMPORTS
%
\documentclass{article} % For LaTeX2e
\usepackage{iclr2022_conference,times}
\input{../math_commands.tex}

\newcommand{\apsname}{Final Report}
\newcommand{\gpnumber}{26}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage[table]{xcolor}

\usepackage{graphicx} % Required for \resizebox
\usepackage{placeins} % Required for \FloatBarrier

%
%  TITLE AND AUTHORS
%
\title{Deep learning approach to  \\ 
mushroom species classification}

\author{Yanni Alan Alevras  \\
Student\# 1009330706 \\
\texttt{yanni.alevras@mail.utoronto.ca} \\
\And
Nicholas Biancolin  \\
Student\# 1009197726 \\
\texttt{n.biancolin@mail.utoronto.ca} \\
\AND
Eric Liu  \\
Student\# 1009098450 \\
\texttt{ey.liu@mail.utoronto.ca} \\
\And
Jason Ruixuan Zhang \\
Student\# 1008997631 \\
\texttt{jasonrx.zhang@mail.utoronto.ca} \\
\AND
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy

%
%   DOCUMENT STARTS HERE
%

\begin{document}
\maketitle

\section{Introduction}
\label{sec:introduction}

Fungi identification is an increasingly critical task, with implications in food security, industrial use, conservation efforts, and biosafety. However, visual and image classification of fungi is a difficult task due to the wide variety of species \citep{LuckingAimeEtAl.UnambiguousIdentificationFungi.2020}.

Nearly 87\% of Ontario's land is Crown Land, filled with expansive forests and a diverse array of plant species. Among these, mushrooms stand out as both common and challenging to identify, due to their wide variety and subtle differences between species. Thus, identification is particularly important because some species are edible and others are highly poisonous, which could pose a risk to foragers and nature enthusiasts.

Our project developed a deep learning model based on a convolutional architecture to accurately identify macrofungi (fungi with large bodies) based on their genus. Deep learning formed a suitable choice for this task, as it constitutes a powerful and accurate method for image recognition and classification tasks, including in ecological settings \citep{SchneiderTaylorEtAl.PresentFutureApproaches.2019}.

We used the MIND.Funga dataset described in \cite{Drechsler-SantosKarstedtEtAl.MINDFunga.2023}, which has approximately 17 000 images of more than 500 species of fungi spanning 216 genera. This dataset is well-suited for our project, as it is built primarily for use in deep classification models. Images are also curated to be of a high quality and are labelled by species.

Our code can be found at this \href{https://github.com/nbiancolin/convolutional-neural-nuts}{GitHub repository}. It contains Jupyter notebooks with pre-processing, baseline, main model, and evaluation code.

\section{Background \& Related Work}
\label{sec:background}

\section{Data Processing}
\label{sec:data_processing}
% TODO: fill out number of classes
The original dataset contains images of 509 classes. Many species have a small number of images associated with them, insufficient for training a deep learning model. To mitigate this, we grouped images into larger "buckets" based on their genus to reduce the number of classes to XX classes, which increases the number of images per class. Genera form an ideal way to group images together, as species of the same genus share physical characteristics and the same root name (first word in the species name) \citep{HollisterCaiEtAl.UsingComputerVision.2023}.

Once this combination is done, we take the top 10 genera with the most images to form an intermediate dataset of 6131 images. We then randomly split the dataset into training, validation, and testing sets with a 75\%, 15\%, and 10\% split, respectively. This split ensures the model has enough data to train on, while ensuring validation and testing better reflects its performance.

Finally, we apply data augmentation techniques described by \cite{ShortenKhoshgoftaar.SurveyImageData.2019} solely to the training set, to avoid poisoning the validation and testing sets. An example of this applied to one image is described in Figure \ref{fig:augmentation}. We applied flips, rotations (90, 180, 270 degrees), Gaussian noise, and random erasing (of black rectangular regions). These manipulations were chosen as they would not alter the image's quality or classification. As mushrooms are identifiable based on visual traits, preserving key details is necessary to distinguish between genera. As our model is agnostic to colour, colour space transformations were not applied.

Our final dataset contains 16 962 training images, 3379 validation images, and 2324 testing images. To generate a usable set, we save a comma-separated value (CSV) file for each set, containing the image path and corresponding label. We also serialize the dataset into a binary Pickle object.

\FloatBarrier
\begin{figure}[h]
    \begin{center}
    \includegraphics[width=0.9\textwidth]{figures/data_augmentation_example.jpg}
    \end{center}
    \caption{Data augmentation techniques applied to a sample image}
    \label{fig:augmentation}
\end{figure}
\FloatBarrier

\section{Architecture}
\label{sec:architecture}
Our primary model relies on two distinct sections: a non-tunable transfer learning section, and a tunable convolutional and fully-connected section. Our data pre-processing and model training pipeline is described in Figure \ref{fig:pipeline}.

We also use cross entropy loss, as it is a standard loss function for multi-class classification tasks. We also use a stochastic gradient optimizer, with a momentum of 0.8, weight decay of 0.001, and learning rate of 0.001. Our training flow uses 100 epochs with a batch size of 64.

\FloatBarrier
\begin{figure}[h]
    \begin{center}
    \includegraphics[width=0.9\textwidth]{figures/modelArchitecture_drawing.png}
    \end{center}
    \caption{Primary model processing and training pipeline}
    \label{fig:pipeline}
\end{figure}
\FloatBarrier

\subsection{Non-Tunable Section}
\label{sec:non_tunable_section}
We use AlexNet for high-level feature extraction, sourced from PyTorch's pre-trained models. AlexNet is a well-known convolutional architecture for image classification tasks \citep{NIPS2012_c399862d}. Since the model needs to differentiate between mushrooms with similar appearances, AlexNet excels in extracting the fine features that set them apart.

Its architecture is described in Figure \ref{fig:alexnet}. It processes a $3\times 244\times 244$ input image, and the feature extraction outputs a $256\times 6\times 6$ tensor. There are five convolutional layers and three pooling layers \citep{NIPS2012_c399862d}.

\FloatBarrier
\begin{figure}[h]
    \begin{center}
    \includegraphics[width=0.9\textwidth]{figures/AlexNetStructure.png}
    \end{center}
    \caption{AlexNet architecture}
    \label{fig:alexnet}
\end{figure}
\FloatBarrier

\subsection{Tunable Section}
\label{sec:tunable_section}
To make the model specific to our team's project, we add an additional convolutional layer, which outputs a $128\times6\times6$ tensor. After this layer, the output is flattened and passed through three fully-connected layers with ReLU activation functions. The fully-connected layers have 256, 128, and 10 neurons, respectively, which matches the number of output classes in our dataset.

In total, there are $5+3$ layers in the non-tunable section, and $1+3$ layers in the tunable section, making our model architecture 12 layers in total.

\section{Baseline Model}
\label{sec:baseline}
Our baseline model is a random forests classifier trained with scikit-learn on the same dataset as our deep learning model. Decision tree classifiers, like random forests, are generally well-suited for multiclass problems \citep{GallRazaviEtAl.IntroductionRandomForests.2012}. We follow similar processing steps as the primary model, including a train-test split of 75\%-25\%, feature extraction, training, and evaluation. We do not apply data augmentation, as this benefits CNNs more.

We apply feature extraction on grayscale images with histogram of oriented gradients (HOG), an edge extraction descriptor algorithm \citep{N.DalalB.Triggs.HistogramsOrientedGradients.2005} and computed with scikit-image. As random forests does not automatically perform feature extraction (compared to deep convolutional networks), this step is done to provide the model with a more effective set of features to learn from. We compute HOG features on both the training and testing set with default parameters, in line with the process outlined by \cite{Dutta.RandomForestImage.2024}. We also use default hyperparameters for training, with the exception of the number of estimators, set to 500, which suggests is an optimal number of trees for increasing accuracy.

Our final baseline model achieves an accuracy of 61\%. We achieve an average precision of 0.7, recall of 0.51, and F1 score of 0.54.

\section{Quantitative Results}
\label{sec:quantitative_results}

\FloatBarrier
\begin{figure}[h]
    \begin{center}
    \includegraphics[width=0.9\textwidth]{figures/trainValidationLoss_finalModel.png}
    \end{center}
    \caption{Train vs validation loss: final model}
    \label{fig:finalModelResult}
\end{figure}
\FloatBarrier

The team achieved a testing accuracy of 81.4\% with our final model. More than a 16\% increase from the progress report.

During model training, the group prioritized validation loss over training loss, as validation loss serves a more unbiased meaure of the model's performance, brining better generalization. After training with a unique set of hyperparameters, we select epochs with low validation loss, saving the models then run the testing set to determine a testing accuracy. Finally, the group selected the model with the highest testing accuracy.

\FloatBarrier
\begin{figure}[h]
    \begin{center}
    \includegraphics[width=0.9\textwidth]{figures/validationLossGraph.png}
    \end{center}
    \caption{Train vs validation loss: progress report}
    \label{fig:progressReportResult}
\end{figure}
\FloatBarrier

From the above graph, the validation curve does not overfit immediately as the one seen in the progress report. This improvement is due data augmentation, weight decay, dropout, and hyperparemeter tuning discussed above and in previous sections. 

Final Hyperparameters:
\begin{itemize}
    \item Learning rate: 0.0006
    \item NumEpoch = 101, chosen epoch for model = 90
    \item Batch size = 64
    \item Weight decay = 0.003
\end{itemize}

\section{Qualitative Results}
\label{sec:qualitative_results}

\section{Evaluation on New Data}
\label{sec:evaluation}

\section{Discussion}
\label{sec:discussion}

\section{Ethical Considerations}
\label{sec:ethics}
Many machine learning models rely on the "unauthorized uses of copyrighted" data for training purposes. This broadly results in infringement of the intellectual property rights and personal interests of the data owners \citep{Sobel.TaxonomyTrainingData.2021}. The dataset used in this project, MIND.Funga, is publicly licensed under a permissive copyleft license (CC BY NC 3.0) \citep{Drechsler-SantosKarstedtEtAl.MINDFunga.2023}. Images were sourced from contributions from the public and academics. However, the organizing project does not disclose that public contributions will be released under this license \citep{Drechsler-SantosKarstedtEtAl.MINDFunga.2023}, suggesting that the dataset may contain images that were not intended to be shared under this license as a result of inadequate informed consent.

The dataset also mostly contains images of tropical fungi from Brazil, with a small subset (approximately 500 out of 14 000) of the globally recorded number of fungi species \citep{LuckingAimeEtAl.UnambiguousIdentificationFungi.2020}. This results in a model that is primarily representative of Brazilian fungi species. As some fungi species can be poisonous, any practical real-world applications of our model may result in misclassifications, which may result in harm to users that rely on the model. This is a significant ethical concern, as the model may be used by individuals who are not experts in mycology, and may not be able to identify the potential risks of consuming certain fungi species.

\label{last_page}

\bibliography{final_report}
\bibliographystyle{iclr2022_conference}

\end{document}
