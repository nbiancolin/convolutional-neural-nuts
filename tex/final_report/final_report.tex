% !TEX root = final_report.tex
%
%  PACKAGE IMPORTS
%
\documentclass{article} % For LaTeX2e
\usepackage{iclr2022_conference,times}
\input{../math_commands.tex}

\newcommand{\apsname}{Final Report}
\newcommand{\gpnumber}{26}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage[table]{xcolor}
\usepackage{subfigure}

\usepackage{pgfplots}
\pgfplotsset{compat=1.17}

\usepackage{graphicx} % Required for \resizebox
\usepackage{placeins} % Required for \FloatBarrier

%
%  TITLE AND AUTHORS
%
\title{Deep learning approach to  \\ 
mushroom species classification}

\author{Yanni Alan Alevras  \\
Student\# 1009330706 \\
\texttt{yanni.alevras@mail.utoronto.ca} \\
\And
Nicholas Biancolin  \\
Student\# 1009197726 \\
\texttt{n.biancolin@mail.utoronto.ca} \\
\AND
Eric Liu  \\
Student\# 1009098450 \\
\texttt{ey.liu@mail.utoronto.ca} \\
\And
Jason Ruixuan Zhang \\
Student\# 1008997631 \\
\texttt{jasonrx.zhang@mail.utoronto.ca} \\
\AND
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy

%
%   DOCUMENT STARTS HERE
%

\begin{document}
\maketitle

\section{Introduction}
\label{sec:introduction}

Fungi identification is an increasingly critical task, with implications in food security, industrial use, conservation efforts, and biosafety. However, visual and image classification of fungi is a difficult task due to the wide variety of species \citep{LuckingAimeEtAl.UnambiguousIdentificationFungi.2020}.

Nearly 87\% of Ontario's land is Crown Land, filled with expansive forests and a diverse array of plant species. Among these, mushrooms stand out as both common and challenging to identify, due to their wide variety and subtle differences between species. Thus, identification is particularly important because some species are edible and others are highly poisonous, which could pose a risk to foragers and nature enthusiasts.

Our project developed a deep learning model based on a convolutional architecture to accurately identify macrofungi (fungi with large bodies) based on their genus. Deep learning formed a suitable choice for this task, as it constitutes a powerful and accurate method for image recognition and classification tasks, including in ecological settings \citep{SchneiderTaylorEtAl.PresentFutureApproaches.2019}.

We used the MIND.Funga dataset described in \cite{Drechsler-SantosKarstedtEtAl.MINDFunga.2023}, which has approximately 17 000 images of more than 500 species of fungi spanning 216 genera. This dataset is well-suited for our project, as it is built primarily for use in deep classification models. Images are also curated to be of a high quality and are labelled by species.

Our code can be found at this \href{https://github.com/nbiancolin/convolutional-neural-nuts}{GitHub repository}. It contains Jupyter notebooks with pre-processing, baseline, main model, and evaluation code.

\section{Background \& Related Work}
\label{sec:background}
Previous applications of fungi identification models have prioritized food safety and satisfaction. In Bangladesh, a country with a large mushroom production, a farming method was developed by \cite{RahmanFaruqEtAl.IoTEnabledMushroom.2022} using an ensemble machine learning classifier (including SVMs, k-nearest neighbours, random forests, and naïve Bayes) to predict which mushroom was harvested. This system was designed to remove toxic species that may have grown in the same area. \cite{WangZhengEtAl.AutomaticSortingSystem.2018} created an OpenCV-based algorithm to identify disease, discolouration, freshness, as well as other factors contributing to the commercial quality of a white button mushroom.

Keeping with the theme of food quality, in Taiwan, \cite{LuLiawEtAl.DevelopmentMushroomGrowth.2019} developed a deep convolutional image recognition model to provide an estimate of mushroom growth progress, similar to the underlying architecture of our model. In the Chinese province of Yunnan, \cite{H.ZhaoF.GeEtAl.IdentificationWildMushroom.2021} created a wild mushroom identification system using a CNN to identify edible and medicinal mushrooms due to increasing popular interest in mushrooms.

Other field work includes smartphone applications for recreational use, like ShroomID, which details mushroom species, while providing a headmap of its location and seasonality. This tool provides useful information about a mushroom after it has been identified using a classification model \citep{.ShroomID.2023}.

\section{Data Processing}
\label{sec:data_processing}
The original dataset contains images of 509 classes. Many species have a small number of images associated with them, insufficient for training a deep learning model. To mitigate this, we grouped images into larger "buckets" based on their genus to reduce the number of classes to 216 classes, which increases the number of images per class. Genera form an ideal way to group images together, as species of the same genus share physical characteristics and the same root name (first word in the species name) \citep{HollisterCaiEtAl.UsingComputerVision.2023}.

Once this combination is done, we take the top 10 genera with the most images to form an intermediate dataset of 6131 images. We then randomly split the dataset into training, validation, and testing sets with a 75\%, 15\%, and 10\% split, respectively. This split ensures the model has enough data to train on, while ensuring validation and testing better reflects its performance.

Finally, we apply data augmentation techniques described by \cite{ShortenKhoshgoftaar.SurveyImageData.2019} separately to  datasets, preventing cross-contamination between them. An example of this applied to one image is described in Figure \ref{fig:augmentation}. We applied flips, rotations (90, 180, 270 degrees), Gaussian noise, and random erasing (of black rectangular regions). These manipulations were chosen as they would not alter the image's quality or classification. As mushrooms are identifiable based on visual traits, preserving key details is necessary to distinguish between genera. As our model is agnostic to colour, colour space transformations were not applied.

Our final dataset contains 16 962 training images, 3379 validation images, and 2324 testing images. To generate a usable set, we save a comma-separated value (CSV) file for each set, containing the image path and corresponding label. We also serialize the dataset into a binary Pickle object.

\FloatBarrier
\begin{figure}[h]
    \begin{center}
    \includegraphics[width=0.9\textwidth]{figures/data_augmentation_example.jpg}
    \end{center}
    \caption{Data augmentation techniques applied to a sample image}
    \label{fig:augmentation}
\end{figure}
\FloatBarrier

\section{Architecture}
\label{sec:architecture}
Our primary model consists of two distinct sections: a non-tunable transfer learning section and a tunable section made up of convolutional and fully-connected layers. Our data pre-processing and model training pipeline is described in Figure \ref{fig:pipeline}.

We use cross entropy loss, as it is a standard loss function for multi-class classification tasks. We also use a stochastic gradient optimizer, with a momentum of 0.8, weight decay of 0.001, and learning rate of 0.001. Our training flow uses 100 epochs with a batch size of 64.

\FloatBarrier
\begin{figure}[h]
    \begin{center}
    \includegraphics[width=0.9\textwidth]{figures/finalArch.png}
    \end{center}
    \caption{Primary model processing and training pipeline}
    \label{fig:pipeline}
\end{figure}
\FloatBarrier

\subsection{Non-Tunable Section}
\label{sec:non_tunable_section}
We use AlexNet for high-level feature extraction, sourced from PyTorch's pre-trained models. AlexNet is a well-known convolutional architecture for image classification tasks \citep{NIPS2012_c399862d}. Since the model needs to differentiate between mushrooms with similar appearances, AlexNet excels in extracting the fine features that set them apart.

Its architecture is described in Figure \ref{fig:alexnet}. It processes a $3\times 244\times 244$ input image, and the feature extraction outputs a $256\times 6\times 6$ tensor. There are five convolutional layers and three pooling layers \citep{NIPS2012_c399862d}.

\FloatBarrier
\begin{figure}[h]
    \begin{center}
    \includegraphics[width=0.9\textwidth]{figures/AlexNetStructure.png}
    \end{center}
    \caption{AlexNet architecture}
    \label{fig:alexnet}
\end{figure}
\FloatBarrier

\subsection{Tunable Section}
\label{sec:tunable_section}
To make the model specific to our team's project, we add an additional convolutional layer, which outputs a $128\times6\times6$ tensor. After this layer, the output is flattened and passed through three fully-connected layers with ReLU activation functions. The fully-connected layers have 256, 128, and 10 neurons, respectively, which matches the number of output classes in our dataset.

In total, there are $5+3$ layers in the non-tunable section, and $1+3$ layers in the tunable section, making our model architecture 12 layers in total.

\section{Baseline Model}
\label{sec:baseline}
Our baseline model is a random forests classifier trained with scikit-learn on the same dataset as our deep learning model. Decision tree classifiers, like random forests, are generally well-suited for multiclass problems \citep{GallRazaviEtAl.IntroductionRandomForests.2012}. We follow similar processing steps as the primary model, including a train-test split of 75\%-25\%, feature extraction, training, and evaluation. We do not apply data augmentation, as this benefits CNNs more.

We apply feature extraction on grayscale images with histogram of oriented gradients (HOG), an edge extraction descriptor algorithm \citep{N.DalalB.Triggs.HistogramsOrientedGradients.2005} and computed with scikit-image. As random forests does not automatically perform feature extraction (compared to deep convolutional networks), this step is done to provide the model with a more effective set of features to learn from. We compute HOG features on both the training and testing set with default parameters, in line with the process outlined by \cite{Dutta.RandomForestImage.2024}. We also use default hyperparameters for training, with the exception of the number of estimators, set to 500, which suggests is an optimal number of trees for increasing accuracy.

Our final baseline model achieves an accuracy of 61\%. We achieve an average precision of 0.7, recall of 0.51, and F1 score of 0.54.

\section{Quantitative Results}
\label{sec:quantitative_results}

\FloatBarrier
\begin{figure}[h]
    \centering
    \subfigure[Train vs validation loss: final model]{%
        \includegraphics[width=0.45\textwidth]{figures/trainValidationLoss_finalModel.png}
        \label{fig:finalModelResult}
    }
    \hfill
    \subfigure[Train vs validation loss: progress report]{%
        \includegraphics[width=0.45\textwidth]{figures/validationLossGraph.png}
        \label{fig:progressReportResult}
    }
    \caption{Comparison of training and validation loss for the final model and progress report}
    \label{fig:sideBySideGraphs}
\end{figure}
\FloatBarrier

The team achieved a testing accuracy of 81.4\% with our final model. More than a 16\% increase from the progress report.

During model training, the group prioritized validation loss over training loss, as validation loss serves a more unbiased meaure of the model's performance, brining better generalization. After training with a unique set of hyperparameters, we select epochs with low validation loss, saving the models then run the testing set to determine a testing accuracy. Finally, the group selected the model with the highest testing accuracy.

From the above final model graph, the validation curve does not overfit immediately as the one seen in the progress report. This improvement is due to data augmentation, weight decay, dropout, and hyperparemeter tuning discussed above and in previous sections. 

Our final hyperparameters were:
\begin{itemize}
    \item Learning rate: 0.0006
    \item Number of epochs: 101, chosen epoch for model: 90
    \item Batch size: 64
    \item Weight decay: 0.003
\end{itemize}

\section{Qualitative Results}
\label{sec:qualitative_results}

To better understand what our team's model can achieve, the group done qualitative analysis on the final model. Firstly, we created a customized function accepting images and predicts the genus to which it belongs to. To test the function, we selected random genus names chosen from the possible categories, and then chose a random image from the test set belonging to each selected genus.

\FloatBarrier
\begin{figure}[h]
    \centering
    \subfigure[Qualitative Model: Successful prediction]{%
        \includegraphics[width=0.45\textwidth]{figures/qualitativeResultImg1.png}
        \label{fig:finalModelResult}
    }
    \hfill
    \subfigure[Qualitative Model: Unsuccessful prediction]{%
        \includegraphics[width=0.45\textwidth]{figures/qualitativeResultImg2.png}
        \label{fig:progressReportResult}
    }
    \caption{Comparison of successful and unsuccessful predictions}
    \label{fig:sideBySideGraphs}
\end{figure}
\FloatBarrier

In the top of the above left figure, the team generated a random genus name and randomly selected an associated image. The image is then passed through the function, which returns the prediction. In the above sample, the prediction matches the generated name, showcasing that the model successully identified the genus. The output tensor for the model's prediction is also displayed on the figure, showing the probability distribution of the image belonging to each of the classes.

In total, the team tested five sample outputs from the model, with three out of the five samples successully identified. 

The right figure above shows one of the wrong predictions made by the model. The two incorrect predictions share some similarities. Firstly, the model is more likely to identify an image if it contains more defining characteristics, such as unique colors, with the fungi zoomed-in and centered. Another common reason for the false prediction is when the image resembles those in other genuses; a common occurrence in our data set.

\FloatBarrier
\begin{figure}[h]
    \begin{center}
    \includegraphics[width=0.8\textwidth]{figures/qualitativeResultImg3.png}
    \end{center}
    \caption{Qualitative Model: Successful prediction}
    \label{fig:goodPrediction2}
\end{figure}
\FloatBarrier

The above figure shows another correct prediction made by the team’s model, supporting the claim that images with unique colors, patterns, and identifiable characteristics has a higher chance of being predicted correctly.

\section{Evaluation on New Data}
\label{sec:evaluation}
We also evaluated our model on data collected from outside the MIND.Funga dataset. We collected 26 images of our dataset's genera, with about 2-3 images per genus. This was collected from Wikimedia Commons, an online repository of permissively-licensed images. We took special care to ensure that the images were not included in the original dataset. As MIND.Funga is primarily made up of images from the public and academics, we are confident that no images from Wikimedia Commons were included in the original dataset \citep{Drechsler-SantosKarstedtEtAl.MINDFunga.2023}.

\begin{figure}[h]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            ybar,
            bar width=15pt,
            ymin=0, ymax=90,
            ylabel={Testing Accuracy (\%)},
            symbolic x coords={Dataset Data, New Data},
            xtick=data,
            nodes near coords,
            enlarge x limits=0.5,
            legend style={at={(0.5,-0.15)},
                anchor=north,legend columns=-1},
            xlabel={},
            width=0.7\textwidth,
            height=0.5\textwidth
            ]
            \addplot coordinates {(Dataset Data, 81.4) (New Data, 65)};
            \addplot coordinates {(Dataset Data, 55) (New Data, 8)};
            \legend{Final Model, Baseline Model}
        \end{axis}
    \end{tikzpicture}
    \caption{Comparison of Testing Accuracy: Final Model vs. Baseline Model}
    \label{fig:testingAccuracyComparison}
\end{figure}

Notably, our baseline model performs significantly worse than our deep learning model, with correct predictions on only 8\% of new images. It overpredicts the genus \textit{Hygrocybe}, the second most frequent genus in the dataset, on 61\% of images. This may be a result of the baseline model's lack of robustness with non-curated data, and largely suggests the rigidity of non-deep learning models without excessive hyperparameter tuning in handling image classification tasks.

\FloatBarrier
\begin{figure}[h]
    \centering
    \subfigure[Cookeina speciosa from dataset]{%
        \includegraphics[width=0.45\textwidth]{figures/cookeina_dataset.jpg}
        \label{fig:finalModelResult}
    }
    \hfill
    \subfigure[Cookeina speciosa from Wikimedia commons]{%
        \includegraphics[width=0.45\textwidth]{figures/cookeina_newdata.jpg}
        \label{fig:progressReportResult}
    }
    \caption{Comparison of dataset data and new data}
    \label{fig:sideBySideGraphs}
\end{figure}
\FloatBarrier

Our main model, successfully identifies 65\% of the new images. We believe this reduction in accuracy is attributable to the new images varying in background complexity, and number of mushrooms per image. For instance, with \textit{Cookeina speciosa} (depicted in \ref{fig:sideBySideGraphs}), all of the original dataset's images were of one mushroom (with a few having a second or third mushroom cut off). However, when sourcing the data, it was difficult to find similar images with a single mushroom in the centre of the image, with most having multiple mushrooms or a distracting background. We believe this is the cause of a lower-than-expected accuracy, due to the nature of the data our model was trained on compared with the new data. A project with data that has less opportunity for similar variations may not face this issue.

\section{Discussion}
\label{sec:discussion}

Our baseline model incurred an accuracy of 61\%, while our main model improved the accuracy by 20\%, totaling 81.4\% testing accuracy. Since the progress report, we have implemented data augmentation, to articifially increase the size of our dataset, and optimized which types of augmentations best benefit the model. This was paired with several rounds of hyperparameter changes, with varying numbers of epochs and learning rates. We would have liked our final model to have a higher accuracy, but we are satisfied with our results, especially since we are comparing between 10 classes.

An area in which we believe we could have improved is in our evaluation on new data. As many of the new images collected had different environments as the ones from our dataset, there were some discrepencies between the new data and test accuracy for the model. This is especially apparent with the baseline model, while the main model performed satisfactorily. This is partly attributable to the simplicity of the baseline model, which lacks the same fine-tuning that the main model has.

Overall, we are satisfied with the results of our model. If this project were to continue, one area that we would like to improve is the amount of genera our model can predict. One limiting factor for our model was the lack of available data for each genus, hence the low limit. We would also be interested in incorporating species-level classification, as many species within a genera share many physical characteristics. Another interesting application is to classify an unidentified mushroom, and try to determine a genera for it, even if that mushroom's species has not been trained on in our model. Quantitatively, our model still requires broad improvements in accuracy and generalization.

This was an informative and interesting project to introduce ourselves into deep learning, especially in a sub-field like computer vision, which is an increasingly frequent application in everyday life.

\section{Ethical Considerations}
\label{sec:ethics}
Many machine learning models rely on the "unauthorized uses of copyrighted" data for training purposes. This broadly results in infringement of the intellectual property rights and personal interests of the data owners \citep{Sobel.TaxonomyTrainingData.2021}. The dataset used in this project, MIND.Funga, is publicly licensed under a permissive copyleft license (CC BY NC 3.0) \citep{Drechsler-SantosKarstedtEtAl.MINDFunga.2023}. Images were sourced from contributions from the public and academics. However, the organizing project does not disclose that public contributions will be released under this license \citep{Drechsler-SantosKarstedtEtAl.MINDFunga.2023}, suggesting that the dataset may contain images that were not intended to be shared under this license as a result of inadequate informed consent.

The dataset also mostly contains images of tropical fungi from Brazil, with a small subset (approximately 500 out of 14 000) of the globally recorded number of fungi species \citep{LuckingAimeEtAl.UnambiguousIdentificationFungi.2020}. This results in a model that is primarily representative of Brazilian fungi species. As some fungi species can be poisonous, any practical real-world applications of our model may result in misclassifications, which may result in harm to users that rely on the model. This is a significant ethical concern, as the model may be used by individuals who are not experts in mycology, and may not be able to identify the potential risks of consuming certain fungi species.

\label{last_page}

\bibliography{final_report}
\bibliographystyle{iclr2022_conference}

\end{document}
