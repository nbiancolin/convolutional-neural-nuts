{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substitute for own data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully retrieved csv file\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\eric7\\OneDrive\\Desktop\\aps360Project\\convolutional-neural-nuts\\data\\set.csv')\n",
    "print(\"successfully retrieved csv file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IN CSV files, formate as [path:genus]\n",
    "Created a dictionary that has keys as genus, and items as image paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby('Genus')['Path'].apply(list).to_dict()\n",
    "\n",
    "mapping = {key: np.array(value) for key, value in grouped.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of data\n",
    "- help decide cnn approach\n",
    "- all data in a numpy dictionary called mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of classes: 70\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of classes: {len(mapping)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class name: Amanita, number of images: 130\n",
      "Class name: Amauroderma, number of images: 133\n",
      "Class name: Antrodia, number of images: 78\n",
      "Class name: Aurantiopileus, number of images: 91\n",
      "Class name: Auricularia, number of images: 389\n",
      "Class name: Boletinellus, number of images: 96\n",
      "Class name: Brunneocorticium, number of images: 108\n",
      "Class name: Calocera, number of images: 125\n",
      "Class name: Calvatia, number of images: 99\n",
      "Class name: Camillea, number of images: 112\n",
      "Class name: Chlorophyllum, number of images: 79\n",
      "Class name: Clavaria, number of images: 209\n",
      "Class name: Cookeina, number of images: 719\n",
      "Class name: Coprinellus, number of images: 228\n",
      "Class name: Cordierites, number of images: 80\n",
      "Class name: Coriolopsis, number of images: 92\n",
      "Class name: Cotylidia, number of images: 112\n",
      "Class name: Cyathus, number of images: 218\n",
      "Class name: Cymatoderma, number of images: 117\n",
      "Class name: Cyptotrama, number of images: 87\n",
      "Class name: Dacryopinax, number of images: 181\n",
      "Class name: Dicephalospora, number of images: 95\n",
      "Class name: Encoelia, number of images: 131\n",
      "Class name: Entoloma, number of images: 486\n",
      "Class name: Erioscyphella, number of images: 99\n",
      "Class name: Favolus, number of images: 199\n",
      "Class name: Filoboletus, number of images: 195\n",
      "Class name: Flaviporus, number of images: 93\n",
      "Class name: Fomitiporia, number of images: 240\n",
      "Class name: Funalia, number of images: 86\n",
      "Class name: Fuscoporia, number of images: 210\n",
      "Class name: Ganoderma, number of images: 140\n",
      "Class name: Geastrum, number of images: 346\n",
      "Class name: Gymnopus, number of images: 80\n",
      "Class name: Hexagonia, number of images: 162\n",
      "Class name: Hydropus, number of images: 144\n",
      "Class name: Hygroaster, number of images: 125\n",
      "Class name: Hygrocybe, number of images: 979\n",
      "Class name: Hymenochaete, number of images: 241\n",
      "Class name: Ionomidotis, number of images: 91\n",
      "Class name: Lentinus, number of images: 108\n",
      "Class name: Leucocoprinus, number of images: 131\n",
      "Class name: Marasmiellus, number of images: 102\n",
      "Class name: Marasmius, number of images: 1122\n",
      "Class name: Microglossum, number of images: 133\n",
      "Class name: Mycocitrus, number of images: 90\n",
      "Class name: Myriostoma, number of images: 100\n",
      "Class name: Ophiocordyceps, number of images: 485\n",
      "Class name: Oudemansiella, number of images: 466\n",
      "Class name: Panaeolus, number of images: 77\n",
      "Class name: Panus, number of images: 209\n",
      "Class name: Penicilliopsis, number of images: 81\n",
      "Class name: Phallus, number of images: 345\n",
      "Class name: Phillipsia, number of images: 163\n",
      "Class name: Pleurotus, number of images: 166\n",
      "Class name: Pluteus, number of images: 243\n",
      "Class name: Pycnoporus, number of images: 237\n",
      "Class name: Rigidoporus, number of images: 104\n",
      "Class name: Schizophyllum, number of images: 252\n",
      "Class name: Scutellinia, number of images: 120\n",
      "Class name: Steccherinum, number of images: 97\n",
      "Class name: Stereum, number of images: 133\n",
      "Class name: Tetrapyrgos, number of images: 157\n",
      "Class name: Thamnomyces, number of images: 85\n",
      "Class name: Trametes, number of images: 318\n",
      "Class name: Trichaptum, number of images: 82\n",
      "Class name: Trogia, number of images: 88\n",
      "Class name: Volvariella, number of images: 78\n",
      "Class name: Xeromphalina, number of images: 146\n",
      "Class name: Xylaria, number of images: 261\n",
      "More than 75: 70\n",
      "Less Than 75: 0\n",
      "Total Images 13804\n"
     ]
    }
   ],
   "source": [
    "LessThan75 = 0\n",
    "MoreThan75 = 0\n",
    "totalCount = 0\n",
    "for dict_keys in mapping.keys():\n",
    "    totalCount += len(mapping[dict_keys])\n",
    "    if len(mapping[dict_keys]) > 75:\n",
    "        print(f\"Class name: {dict_keys}, number of images: {len(mapping[dict_keys])}\")\n",
    "        MoreThan75 += 1\n",
    "    else:\n",
    "        LessThan75 += 1\n",
    "print(f\"More than 75: {MoreThan75}\")\n",
    "print(f\"Less Than 75: {LessThan75}\")\n",
    "print(f\"Total Images {totalCount}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "#variable to track number of clases\n",
    "n = len(mapping)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN model with transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "#need to pip install torch here\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preLearnedFungaV1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(preLearnedFungaV1, self).__init__()\n",
    "        self.name = \"fungaV1\"\n",
    "        self.fc1 = nn.Linear(256*6*6, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, n)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = x.squeeze(1)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training, validation and testing\n",
    "- apply the transfer learning here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to install torchvision\n",
    "from torch.utils.data import TensorDataset\n",
    "from torchvision import transforms\n",
    "#adjust to 224 dimension (input to alexnet)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((224,224))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eric7\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\eric7\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#use alexnet for now\n",
    "import torchvision.models\n",
    "alexnet = torchvision.models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of classes 70\n",
      "Start Transfer Learning Section: AlexNet feature extract for each class\n",
      "Class 1 out of 70\n",
      "Class 2 out of 70\n",
      "Class 3 out of 70\n",
      "Class 4 out of 70\n",
      "Class 5 out of 70\n",
      "Class 6 out of 70\n",
      "Class 7 out of 70\n",
      "Class 8 out of 70\n",
      "Class 9 out of 70\n",
      "Class 10 out of 70\n",
      "Class 11 out of 70\n",
      "Class 12 out of 70\n",
      "Class 13 out of 70\n",
      "Class 14 out of 70\n",
      "Class 15 out of 70\n",
      "Class 16 out of 70\n",
      "Class 17 out of 70\n",
      "Class 18 out of 70\n",
      "Class 19 out of 70\n",
      "Class 20 out of 70\n",
      "Class 21 out of 70\n",
      "Class 22 out of 70\n",
      "Class 23 out of 70\n",
      "Class 24 out of 70\n",
      "Class 25 out of 70\n",
      "Class 26 out of 70\n",
      "Class 27 out of 70\n",
      "Class 28 out of 70\n",
      "Class 29 out of 70\n",
      "Class 30 out of 70\n",
      "Class 31 out of 70\n",
      "Class 32 out of 70\n",
      "Class 33 out of 70\n",
      "Class 34 out of 70\n",
      "Class 35 out of 70\n",
      "Class 36 out of 70\n",
      "Class 37 out of 70\n",
      "Class 38 out of 70\n",
      "Class 39 out of 70\n",
      "Class 40 out of 70\n",
      "Class 41 out of 70\n",
      "Class 42 out of 70\n",
      "Class 43 out of 70\n",
      "Class 44 out of 70\n",
      "Class 45 out of 70\n",
      "Class 46 out of 70\n",
      "Class 47 out of 70\n",
      "Class 48 out of 70\n",
      "Class 49 out of 70\n",
      "Class 50 out of 70\n",
      "Class 51 out of 70\n",
      "Class 52 out of 70\n",
      "Class 53 out of 70\n",
      "Class 54 out of 70\n",
      "Class 55 out of 70\n",
      "Class 56 out of 70\n",
      "Class 57 out of 70\n",
      "Class 58 out of 70\n",
      "Class 59 out of 70\n",
      "Class 60 out of 70\n",
      "Class 61 out of 70\n",
      "Class 62 out of 70\n",
      "Class 63 out of 70\n",
      "Class 64 out of 70\n",
      "Class 65 out of 70\n",
      "Class 66 out of 70\n",
      "Class 67 out of 70\n",
      "Class 68 out of 70\n",
      "Class 69 out of 70\n",
      "Class 70 out of 70\n"
     ]
    }
   ],
   "source": [
    "#running this takes a long time, only do it after set.csv is fixed\n",
    "dict = {}\n",
    "print(f\"Total number of classes {n}\")\n",
    "print(\"Start Transfer Learning Section: AlexNet feature extract for each class\")\n",
    "iterCount = 1\n",
    "\n",
    "for key, images in mapping.items():\n",
    "    print(f\"Class {iterCount} out of {n}\")\n",
    "    dict[key] = []\n",
    "    for img in images:\n",
    "        newImg = Image.open(img)\n",
    "        newImg = transform(newImg)\n",
    "        feat = alexnet.features(newImg)\n",
    "        dict[key].append(feat)\n",
    "    iterCount += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "#change from str key to numerical key\n",
    "mappingStrToInt = {idx: value for idx, (key, value) in enumerate(dict.items())}\n",
    "print(len(mappingStrToInt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "#numerical dictionary\n",
    "numericalDict = {}\n",
    "for idx, (key, value) in enumerate(dict.items()):\n",
    "    numericalDict[idx] = value\n",
    "print(len(numericalDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running data dictionary\n",
      "Complete Running Data Dictionary\n"
     ]
    }
   ],
   "source": [
    "# Split data into training, validation, and testing sets\n",
    "tData, tLabel = [], []\n",
    "vData, vLabel = [], []\n",
    "testingData, testingLabel = [], []\n",
    "\n",
    "print(\"Running data dictionary\")\n",
    "numIter = 1\n",
    "#0.75 training, 0.15 valdiation, 0.10 for testing\n",
    "for key, values in numericalDict.items():\n",
    "    numTrain = int(len(values) * 0.75)\n",
    "    numVal = int(len(values) * 0.15)\n",
    "\n",
    "    tLabel.extend([key] * numTrain)\n",
    "    vLabel.extend([key] * numVal)\n",
    "    testingLabel.extend([key] * (len(values) - numTrain - numVal))\n",
    "\n",
    "    tData.extend(values[:numTrain])\n",
    "    vData.extend(values[numTrain:numTrain + numVal])\n",
    "    testingData.extend(values[numTrain + numVal:])\n",
    "\n",
    "    numIter += 1\n",
    "print(\"Complete Running Data Dictionary\")\n",
    "tTensor = torch.stack(tData)\n",
    "vTensor = torch.stack(vData)\n",
    "testingTensor = torch.stack(testingData)\n",
    "\n",
    "tLabelTensor = torch.tensor(tLabel)\n",
    "vLabelTensor = torch.tensor(vLabel)\n",
    "testingLabelTensor = torch.tensor(testingLabel)\n",
    "\n",
    "train_set = TensorDataset(tTensor, tLabelTensor)\n",
    "validation_set = TensorDataset(vTensor, vLabelTensor)\n",
    "test_set = TensorDataset(testingTensor, testingLabelTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tTensor shape: torch.Size([10325, 256, 6, 6])\n",
      "vTensor shape: torch.Size([2033, 256, 6, 6])\n",
      "testingTensor shape: torch.Size([1446, 256, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "print(f'tTensor shape: {tTensor.shape}')\n",
    "print(f'vTensor shape: {vTensor.shape}')\n",
    "print(f'testingTensor shape: {testingTensor.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10325\n",
      "2033\n",
      "1446\n"
     ]
    }
   ],
   "source": [
    "#testing if train, validation, and test sets work\n",
    "print(len(train_set))\n",
    "print(len(validation_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#customized data_loading function with variable batch size\n",
    "import torch\n",
    "def dataLoadFunction(BatchSize):\n",
    "  train_load = torch.utils.data.DataLoader(train_set, batch_size = BatchSize, shuffle=True)\n",
    "  validation_load = torch.utils.data.DataLoader(validation_set, batch_size = BatchSize)\n",
    "  testing_load = torch.utils.data.DataLoader(test_set, batch_size = BatchSize)\n",
    "  return train_load, validation_load, testing_load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValidationLoss(net, validation_loader, criterion):\n",
    "  n = len(validation_loader)\n",
    "  totalLoss = 0\n",
    "  for i, data in enumerate(validation_loader):\n",
    "    inputs, labels = data\n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    totalLoss += loss.item()\n",
    "  totalLoss = totalLoss / n\n",
    "  return totalLoss\n",
    "\n",
    "#model name\n",
    "def get_model_name(name, batch_size, lr, epoch):\n",
    "    path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name, batch_size, lr, epoch)\n",
    "    return path\n",
    "\n",
    "#plotting helper function\n",
    "import matplotlib.pyplot as plt\n",
    "#plot image function\n",
    "#modified from lab 2\n",
    "def plot_training_curve(path):\n",
    "    train_loss = np.loadtxt(\"{}_train_loss.csv\".format(path))\n",
    "    val_loss = np.loadtxt(\"{}_val_loss.csv\".format(path))\n",
    "    n = len(train_loss)\n",
    "    #plot model\n",
    "    plt.title(\"Train vs Validation Loss\")\n",
    "    plt.plot(range(1,n+1), train_loss, label=\"Train\")\n",
    "    plt.plot(range(1,n+1), val_loss, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allClasses = []\n",
    "#^need to loop through all classes in mapping (later problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net_TL(net, batchsize=24, learning_rate=0.007, num_epochs=12):\n",
    "\n",
    "  torch.manual_seed(1)\n",
    "\n",
    "  target_classes = allClasses #defined in last part\n",
    "\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.8)\n",
    "\n",
    "  training_loader, validation_loader, _ = dataLoadFunction(batchsize)\n",
    "\n",
    "  train_loss_graph = np.zeros(num_epochs)\n",
    "  val_loss_graph = np.zeros(num_epochs)\n",
    "\n",
    "  n = len(training_loader)\n",
    "  startTime = time.time()\n",
    "  for currepoch in range(num_epochs):\n",
    "    #print(\"New epoch\")\n",
    "    trainLoss = 0\n",
    "    #net.train()\n",
    "    for i, (inputs, labels) in enumerate(training_loader):\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      inputs = torch.tensor(inputs.detach().numpy()).to(inputs.device)\n",
    "      outputs = net(inputs)\n",
    "\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      trainLoss += loss.item()\n",
    "\n",
    "    #print(\"Onto validation\")\n",
    "    #net.eval()\n",
    "    train_loss_graph[currepoch] = trainLoss/n\n",
    "    val_loss_graph[currepoch] = getValidationLoss(net, validation_loader, criterion)\n",
    "\n",
    "    print(f\"Epoch: {currepoch} Training Loss: {train_loss_graph[currepoch]:.2f}, Validation Loss: {val_loss_graph[currepoch]:.2f}, Time: {time.time() - startTime:.2f} seconds\")\n",
    "\n",
    "    model_path = get_model_name(net.name, batchsize, learning_rate, currepoch)\n",
    "    torch.save(net.state_dict(), model_path)\n",
    "  print(\"Total time: \", time.time()-startTime)\n",
    "\n",
    "  path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(net.name, batchsize, learning_rate, num_epochs)\n",
    "  np.savetxt(\"{}_train_loss.csv\".format(path), train_loss_graph)\n",
    "  np.savetxt(\"{}_val_loss.csv\".format(path), val_loss_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net3TL = preLearnedFungaV1()\n",
    "train_net_TL(net3TL, batchsize=36, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot curve here\n",
    "plot_training_curve(\"model_{0}_bs{1}_lr{2}_epoch{3}\".format(\"Lab3Net\", 12, 0.007, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test accuracy function\n",
    "def getAccuracy(net, loader):\n",
    "  correct, count = 0, 0\n",
    "\n",
    "  #no gradient modification\n",
    "  with torch.no_grad():\n",
    "    print(\"Total number of batches: \", len(loader))\n",
    "    for currbatch, (i, corr) in enumerate(loader):\n",
    "      count += corr.shape[0]\n",
    "      print(\"CurrBatch: \", currbatch+1)\n",
    "\n",
    "      correct += (torch.max(net(i), 1)[1] == corr).sum().item()\n",
    "\n",
    "  return correct / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bestNet = preLearnedFungaV1()\n",
    "model_path = get_model_name(\"CHANGENAME\", batch_size=24, lr=0.01, epoch=7)\n",
    "state = torch.load(model_path)\n",
    "bestNet.load_state_dict(state)\n",
    "\n",
    "_,_, testLoad = dataLoadFunction(24)\n",
    "\n",
    "dataLoad = testLoad\n",
    "\n",
    "accuracy = getAccuracy(bestNet, dataLoad)\n",
    "print(\"Test Classification Accuracy: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
